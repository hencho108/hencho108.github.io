---
title: "Predicting High Google Play App Ratings"
author: "[Hendrik Mischo](https://github.com/hendrik-mischo)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  github_document: default
  html_document:
    code_folding: show
    toc: true
    toc_float: true
    number_sections: true
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=4, fig.align="center", echo=TRUE, warning=FALSE, message=FALSE,autodep = TRUE, cache=TRUE)
```

# Introduction

The aim of this project is to see if we can predict a high rating for Google Play Apps given the basic information provided on the Play Store. The supplemental dataset is available on [Kaggle](https://www.kaggle.com/lava18/google-play-store-apps).

I decided to make this a binary classification problem where an app rating can either be "high" if it is higher than 4.0 or "low" if it is below 4.0. I did this because I assume that users usually do not bother too much about whether an app has 4.1 or 4.3 stars. An app rated at least 4.0 is simply good app. Hence, the developer can be happy. If the rating is lower, there is room for improvement.

**Procedure:** \
1. Data Exploration (briefly) and Cleaning \
2. Model Building and Tuning \
3. Evaluation

I will apply Logistic Regression, Random Forest and Boosting. This way we can compare not only tree-based models but also regression. To evalutate the performance of the models, I will look at the area under the curve (AUC), overall classification error and ensure the classification errors per class are more or less balanced.

Let's start with importing the relevant libraries and data.

```{r}
library(tidyverse)  # For data wrangling
library(lubridate)  # To deal with dates
library(h2o)        # ML platform - Used for building models
library(data.table) # To be used with h2o (see documentation of "as.h2o")

# Import the data
data = read.csv("googleplaystore.csv", sep=",")
attach(data)
```

# Data Exploration and Cleaning

First let's have a look at the structure of the data.

```{r}
head(data)
```

**Description of the variables:** </br> \
App - *Application name* </br> \
Category - *Category the app belongs to* </br> \
Rating - *Overall user rating of the app (as when scraped)* </br> \
Reviews - *Number of user reviews for the app (as when scraped)* </br> \
Size - *Size of the app (as when scraped)* </br> \
Installs - *Number of user downloads/installs for the app (as when scraped)* </br> \
Type - *Paid or Free* </br> \
Price - *Price of the app (as when scraped)* </br> \
Content Rating - *Age group the app is targeted at* </br> \
Genres - *An app can belong to multiple genres (apart from its main category).* </br> \
Last Updated - *Date when the app was last updated on Play Store (as when scraped)* </br> \
Current Ver - *Current version of the app available on Play Store (as when scraped)* </br> \
Android Ver - *Min. required Android version (as when scraped)*

Now check the data formats and types.

```{r}
glimpse(data)
```

Let's also have a look at the summary of the data to check if there are any abnormal values.

```{r}
summary(data)
```

Looks like some cleaning is required.

I notice the following issues. </br> \
1) It appears that there are many factor variables that should be numeric. </br> \
2) The units of the sizes of the apps vary (KB and MB). </br> \
3) The installations have a "+" attached at the end. </br> \
4) Price values include the dollar sign. </br> \
5) It may be difficult for some algorithms to deal with the dates. </br> \
6) The current app version and minimum Android version can only be treated as categories as they are.

I will clean the data and replace any abnormal values with NA. For the response variable "Rating" I will assign "High" to ratings of 4.0 or higher and "Low" to lower ratings. 

```{r, warning=FALSE}
# Drop unnecessary columns
data = data[,!names(data) == "App"]

data = data %>%
  mutate(
    # Convert "Varies with device" to NA in Size
    Size = ifelse(grepl("Varies with device", Size), NA, as.character(Size)),
    # Convert MB to KB
    Size = ifelse(grepl("M", Size), as.numeric(gsub("M", "", Size))*1000, as.character(Size)),
    # Drop the "k" in Size
    Size = as.numeric(gsub("k", "", Size)),
    # Drop the "+" in Installs
    Installs = gsub("\\+", "", as.character(Installs)),
    # Drop the "," and convert number of installs to numeric
    Installs = as.numeric(gsub(",", "", Installs)),
    # Convert number of reviews to numeric
    Reviews = as.numeric(Reviews),
    # Drop the "$" and convert Price to numeric
    Price = as.numeric(gsub("\\$", "", as.character(Price))),
    # Convert Last.Updated to categorical
    Last.Updated = mdy(Last.Updated),
    Last.Updated = ifelse(Last.Updated >= as.Date("2018-01-01"), "In 2018",
                   ifelse(Last.Updated < as.Date("2018-01-01") & 
                            Last.Updated >= as.Date("2017-01-01"), "In 2017",
                   ifelse(Last.Updated < as.Date("2017-01-01") & 
                            Last.Updated >= as.Date("2016-01-01"), "In 2016",
                   "Before 2016"))),
    Last.Updated = as.factor(Last.Updated),
    # Convert "Varies with device" to NA in Android.Ver
    Android.Ver = gsub("Varies with device", NA, Android.Ver),
    # Keep version number to 1 decimal only
    Android.Ver = as.numeric(substr(Android.Ver, start = 1, stop = 3)),
    # Convert "Varies with device" to NA in Current.Ver
    Current.Ver = gsub("Varies with device", NA, Current.Ver),
    # Keep version number to 1 decimal only
    Current.Ver = as.numeric(substr(Current.Ver, start = 1, stop = 3)),
    # Convert ratings larger than 5 to NA
    Rating = ifelse(Rating > 5, NA, as.numeric(Rating)),
    # Convert Rating to binary
    Rating = ifelse(Rating >= 4, "High", "Low"),
    Rating = as.factor(Rating)
  ) %>%
  # Drop the strange category "1.9"
  filter(!Category=="1.9")
```

Check if there are any NAs.

```{r}
sapply(data, function(x) sum(is.na(x)))
```

Some variables contain missing values. For "Size" I will substitute the mean size and for "Current.Ver" and "Android.Ver" the median. However, this approach does not work for the response variable "Rating" because the average rating is 4.19, so all missing values would be assigned to the category "High", which could lead to bias. For this reason I decide to drop these observations. Given that the missing values in Rating make up about `r round(sum(is.na(data$Rating))/nrow(data),3)*100`% of the observations we will still have enough training data.

```{r}
data = data %>% 
  # Drop NAs in Rating
  drop_na(Rating) %>%
  mutate(
    # Replace NAs in Size with mean
    Size = ifelse(is.na(Size), mean(Size, na.rm = TRUE), Size),
    # Replace NAs in Andriod.Ver with median
    Android.Ver = ifelse(is.na(Android.Ver), median(Android.Ver, na.rm = TRUE), Android.Ver),
    # Replace NAs in Current.Ver with median
    Current.Ver = ifelse(is.na(Current.Ver), median(Current.Ver, na.rm = TRUE), Current.Ver)
    )
```

Glimpse the clean data.

```{r}
head(data)
```


# 2. Model Building and Tuning

With the preprocessed data we can start building models. For this part I decided to use H2O. H2O is an open-source, distributed, fast, and scalable predictive analytics platform. It supports the most widely used statistical learning algorithms. Here H2O is useful for two reasons. First, it is more potent than the algorithms of the standard packages as that it allows for more parameters and it is faster. Second, it can handle categorical variables with many more categories. This way we can avoid one-hot encoding the variable "Genres", which would significantly increase the number of independent variables as it contains 120 categories.

Now I will set up H2O and create the training, validation and test sets.

```{r, results="hide"}
h2o.init(min_mem_size = "5g") # Start up H2O
h2o.removeAll()               # Clear - just in case the cluster was already running
h2o.no_progress()             # Do not show progress bar

# Covert data to H2O object
data = as.h2o(data)

# Create training, validation and test sets: 0.7, 0.15, 0.15
splits = h2o.splitFrame(data = data, ratios = c(0.7,0.15), seed = 2019)
train = splits[[1]]
valid = splits[[2]]
test = splits[[3]]

# Define features and response variable
Y = "Rating"
X = setdiff(colnames(data), Y)
```

## 2.1. Logistic Regression

First, I establish the baseline performance.

```{r}
glm.base = h2o.glm(y = Y, x = X, 
                   training_frame = train, 
                   family = "binomial"
                   )
h2o.confusionMatrix(glm.base, newdata = valid)
# Get the AUC
h2o.auc(h2o.performance(glm.base, newdata = valid))
```

Now that we know the baseline performance, we can tune the model in more detail. The most important parameters are the alpha, which determines to what extent Ride and Lasso are used, and the shrinkage parameter lambda.

```{r}
hyper_parameters = list(alpha = seq(0,1,0.1))

glm.grid = h2o.grid(algorithm = "glm", 
                     y = Y, x = X,
                     hyper_params = hyper_parameters,
                     search_criteria = list(strategy = "Cartesian"),
                     training_frame = train, 
                     validation_frame = valid,
                     family = "binomial", 
                     lambda_search = TRUE, # automatically search for lambda
                     max_active_predictors = 11,
                     remove_collinear_columns=TRUE,
                     grid_id = "glm_grid"
                     )
glm.grid.sorted = h2o.getGrid("glm_grid", sort_by="auc", decreasing = TRUE)
```

Check how the tuned model performs on the validation set.

```{r}
glm.tuned = h2o.getModel(glm.grid.sorted@model_ids[[1]])
h2o.confusionMatrix(glm.tuned, newdata = valid)
# Get the AUC
h2o.auc(h2o.performance(glm.tuned, newdata = valid))
```

The performance did not improve compared to the base model. It appears that the machine found a better combination of hyper-parameters. 

I will now apply the test data to the base model to fix a final score for Logistic Regression.

```{r}
h2o.confusionMatrix(glm.base, newdata = test)
glm.err = round(h2o.confusionMatrix(glm.base, newdata = test)$Error[3], 3)
glm.auc = round(h2o.auc(h2o.performance(glm.base, newdata = test)), 3)
data.frame(AUC = glm.auc, ClassErr = glm.err)
```

We use 5-fold cross-validation on the entire dataset to confirm that the parameters we found are reasonable.

```{r}
glm.tuned.cv = do.call(h2o.glm,
                      ## update parameters in place
                      {
                        p = glm.tuned@parameters
                        p$model_id = NULL          # do not overwrite the original grid model
                        p$training_frame = data    # use the full dataset
                        p$validation_frame = NULL  # no validation frame
                        p$nfolds = 5               # cross-validation
                        p
                      })
glm.tuned.cv@model$cross_validation_metrics_summary["auc",]
```

We might have overfitted slightly since the AUC varies quite a bit depending on the train/test split.

Let's see how the tree-based models compare.

## 2.2 Random Forest

Again I first establish the baseline performance.

```{r}
rf.base = h2o.randomForest(x = X, y = Y,
                      training_frame = train,
                      seed = 2019)
h2o.confusionMatrix(rf.base, newdata=valid)
# Get the AUC
h2o.auc(h2o.performance(rf.base, newdata = valid))
```

While the overall classification error and the error rate for high ratings are classified relatively well, this model performs rather poorly at classifying low ratings. This may be because the data is somewhat unbalanced. Let's have a look at the fraction of observations in each category.

```{r}
round(table(as.data.frame(train)$Rating) / nrow(as.data.frame(train)), 2)
```

Now I will fit another random forest but this time I add the balance_classes argument to balance the data using undersamping/oversampling.

```{r}
rf.base = h2o.randomForest(x = X, y = Y, 
                      training_frame = train,
                      seed = 2019,
                      balance_classes = TRUE)
h2o.confusionMatrix(rf.base, newdata = valid)
```

The overall error has increased, however the error for a low rating as has improved quite significantly.

Now we can tune the model to see if we can further improve the model. Let's find the optimal number of trees. To do this I will fit a new random forest with many trees and plot the errors.

```{r}
rf = h2o.randomForest(x = X, y = Y, 
                      training_frame = train, 
                      validation_frame = valid,
                      seed = 2019,
                      balance_classes = TRUE,
                      ntrees = 500
                      )
plot(rf, timestep = "number_of_trees", metric="classification_error")
```

The optimal number of trees seems to be somewhere between 20 and 50. In order to determine the exact value, I add the stopping_rounds argument, which stops the fitting process when the moving average of length 10 of the error rate does not improve.

```{r}
rf = h2o.randomForest(x = X, y = Y, 
                      training_frame = train, 
                      validation_frame = valid,
                      seed = 2019,
                      balance_classes = TRUE,
                      stopping_rounds = 10,
                      stopping_metric = "misclassification")
ntrees = rf@model$model_summary$number_of_trees
ntrees
```

Next I will determine the optimal number of variables to be considered at each split.

```{r}
oob.err = double(length(X))
test.err = double(length(X))
test.err.low = double(length(X))
test.err.high = double(length(X))

# Fit a new forest for each value of mtry and save the errors
for(mtry in 1:length(X)){
  rf = h2o.randomForest(x = X, y = Y, 
                      training_frame = train, 
                      validation_frame = valid,
                      seed = 2019,
                      balance_classes = TRUE,
                      ntrees = ntrees,
                      mtries = mtry)
  oob.err[mtry] = h2o.confusionMatrix(rf)$Error[3]
  test.err[mtry] = h2o.confusionMatrix(rf, newdata = valid)$Error[3]
  test.err.low[mtry] = h2o.confusionMatrix(rf, newdata = valid)$Error[2]
  test.err.high[mtry] = h2o.confusionMatrix(rf, newdata = valid)$Error[1]
}
# Plot the result
matplot(1:mtry, cbind(oob.err,test.err,test.err.low,test.err.high), pch=19, type="b", 
        xlab = "Number of Variables", ylab="Classification Error",
        col=c("grey", "blue", "orange", "red"))
legend("topright", legend=c("OOB", "Test", "Class: Low", "Class: High"), pch=19, 
       col=c("grey", "blue", "orange", "red"))
```

It appears that there is a trade-off between overall error rate and error for the "low" class. A good value seems to be 3 variables, which is also the default of the random forest function for our number of features.

```{r}
rf.tuned = h2o.randomForest(x = X, y = Y, 
                            training_frame = train, 
                            validation_frame = valid,
                            seed = 2019,
                            balance_classes = TRUE,
                            ntrees = ntrees,
                            mtries = 3
                            )
h2o.confusionMatrix(rf.tuned, newdata = valid)
# Get the AUC
h2o.auc(h2o.performance(rf.tuned, newdata = valid))
```

Given that the default number of variables also turned out to be the optimal number, it is not surprising that the AUC and the errors did not improve a lot compared to the initial model. However, we managed to achieve the same performance with less trees, i.e. a simpler model.

Let's apply the test data to the base model to fix a final score for Random Forest.

```{r}
h2o.confusionMatrix(rf.tuned, newdata = test)
rf.err = round(h2o.confusionMatrix(rf.tuned, newdata = test)$Error[3], 3)
rf.auc = round(h2o.auc(h2o.performance(rf.tuned, newdata = test)), 3)
data.frame(AUC = rf.auc, ClassErr = rf.err)
```

Let's use 5-fold cross-validation on the entire dataset to confirm that the parameters we found are reasonable.

```{r}
rf.tuned.cv = do.call(h2o.randomForest,
                      ## update parameters in place
                      {
                        p = rf.tuned@parameters
                        p$model_id = NULL          # do not overwrite the original grid model
                        p$training_frame = data    # use the full dataset
                        p$validation_frame = NULL  # no validation frame
                        p$nfolds = 5               # cross-validation
                        p
                      })
rf.tuned.cv@model$cross_validation_metrics_summary["auc",]
```

It seems that the parameters are reasonable as the result does not vary significantly.

## 2.3 Boosting

As before, we check the baseline performance first only providing the required parameters and leaving everything else default.

```{r}
gbm.base = h2o.gbm(x = X, y = Y, 
                   training_frame = train,
                   seed = 2019
                   )
h2o.confusionMatrix(gbm.base, newdata = valid)
# Get the AUC
h2o.auc(h2o.performance(gbm.base, newdata = valid))
```

Let's check if balancing the classes has a noticeable effect.

```{r}
gbm.base = h2o.gbm(x = X, y = Y, 
                   training_frame = train,
                   seed = 2019,
                   balance_classes = TRUE
                   )
h2o.confusionMatrix(gbm.base, newdata = valid)
h2o.auc(h2o.performance(gbm.base, newdata = valid))
```

The effect of balancing the classes does not seem to effect the performance very much like it did with the random forest model. This may be because the classes are only slightly unbalanced.

Next I will tune the model. Here we have more hyper-parameters that we need to tune. For this purpose I will do a random grid search. The tuning approach for this model is based on the article https://www.h2o.ai/blog/h2o-gbm-tuning-tutorial-for-r/. I start tuning the parameter max_depth because of its importance. It is not necessary to start with one, but we can save computing time by finding a range of a particularly important tuning parameter first. 

```{r}
hyper_params = list(max_depth = seq(1,30,1)) 

gbm.grid = h2o.grid(
  # Standard model parameters
  algorithm = "gbm",
  x = X,
  y = Y,
  training_frame = train,
  validation_frame = valid,
  seed = 2019,
  # Hyper parameters
  hyper_params = hyper_params,
  # Search criteria 
  search_criteria = list(strategy = "Cartesian"),
  # Identifier for the grid
  grid_id = "max_depth_grid",
  # Use a sufficient number of trees (limited by early stopping)
  ntrees = 1000,
  # Start with bigger learning rate. Smaller would be better, but learning_rate_annealing will shrink it
  learn_rate = 0.05,
  # Shrink learning_rate by 1% after every tree
  learn_rate_annealing = 0.99,
  # Sample 80% of rows per tree
  sample_rate = 0.8,
  # Sample 80% of columns per split
  col_sample_rate = 0.8,
  # Stop early if AUC does not improve by >= 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",
  # Score every 10 trees to make early stopping reproducible
  score_tree_interval = 10,
  # Balance the classes
  balance_classes = TRUE
)
```

Display the top 5 grid models by decreasing AUC.

```{r}
gbm.grid.sorted = h2o.getGrid("max_depth_grid", sort_by="auc", decreasing = TRUE)
gbm.grid.sorted@summary_table[1:5,]
```

Now find the max_depth range for the top 5 models.

```{r}
topDepths = gbm.grid.sorted@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
```

The max_depth values of `r minDepth` to `r maxDepth` seem to be best suited for this dataset. 

Now that we know a good range for max_depth, we can further tune the other parameters to find the optimal combination.

First I define the parameters and search criteria.

```{r}
hyper_params = list(
  # Set max_depth range determined earlier
  max_depth = seq(minDepth, maxDepth, 1),
  # Search a large space of different parameters
  sample_rate = seq(0.2,1,0.01),
  col_sample_rate = seq(0.2,1,0.01),
  col_sample_rate_per_tree = seq(0.2,1,0.01),
  col_sample_rate_change_per_level = seq(0.9,1.1,0.01),
  min_rows = 2^seq(0,log2(nrow(train))-1,1),
  nbins = 2^seq(4,10,1),
  nbins_cats = 2^seq(4,12,1),
  min_split_improvement = c(0,1e-8,1e-6,1e-4),
  # Try all histogram types 
  histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")
)

search_criteria = list(
  # Random grid search
  strategy = "RandomDiscrete",
  seed = 2019,
  # Limit processing time
  max_runtime_secs = 3600,
  max_models = 100,
  # Early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
  stopping_rounds = 5,
  stopping_metric = "AUC",
  stopping_tolerance = 1e-3
)
```

With the updated and extended hyper-parameters and search criteria we create a new grid.

```{r}
gbm.grid = h2o.grid(
  # Standard model parameters
  algorithm = "gbm",
  x = X,
  y = Y,
  training_frame = train,
  validation_frame = valid,
  seed = 2019,
  # Hyper parameters
  hyper_params = hyper_params,
  # Search criteria 
  search_criteria = search_criteria,
  # Identifier for the grid
  grid_id = "gbm_grid",
  # Use a sufficient number of trees (limited by early stopping)
  ntrees = 1000,
  # Start with bigger learning rate. Smaller would be better, but learning_rate_annealing will shrink it
  learn_rate = 0.05,
  # Shrink learning_rate by 1% after every tree
  learn_rate_annealing = 0.99,
  # Stop early if AUC does not improve by >= 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",
  # Score every 10 trees to make early stopping reproducible
  score_tree_interval = 10,
  # Balance the classes
  balance_classes = TRUE
)
```

Again display the top 5 models to see investigate their AUC and parameters.

```{r}
gbm.gird.sorted = h2o.getGrid("gbm_grid", sort_by="auc", decreasing = TRUE)
gbm.gird.sorted@summary_table[1:5,]
```

Now let's evaluate the model performance on the test set to give it a final score.

```{r}
gbm.tuned = h2o.getModel(gbm.gird.sorted@model_ids[[1]])
h2o.confusionMatrix(gbm.tuned, newdata = test)
gbm.err = round(h2o.confusionMatrix(gbm.tuned, newdata = test)$Error[3], 3)
gbm.auc = round(h2o.auc(h2o.performance(gbm.tuned, newdata = test)), 3)
data.frame(AUC = gbm.auc, ClassErr = gbm.err)
```

I will use cross-validation on the entire dataset to confirm that the parameters we found are reasonable.

```{r}
gbm.tuned.cv = do.call(h2o.gbm,
                       ## update parameters in place
                       {
                         p = gbm.tuned@parameters
                         p$model_id = NULL          # do not overwrite the original grid model
                         p$training_frame = data    # use the full dataset
                         p$validation_frame = NULL  # no validation frame
                         p$nfolds = 5               # cross-validation
                         p
                        })
gbm.tuned.cv@model$cross_validation_metrics_summary["auc",]
```

We can see that the mean of the cross-validated AUC and the test AUC are very similar and that the standard deviation is rather small. This is a good sign as it indicates that we did not over or underfit a lot.

# 3. Evaluation

Having built and tuned the models, we now want to evaluate and compare their results to decide which is the best.

Let's compare the final performance of the models on the test set side by side.

```{r}
data.frame(GLM = c(glm.auc, glm.err),
           RF = c(rf.auc, rf.err),
           GBM = c(gbm.auc, gbm.err),
           row.names = c("AUC", "Class. Err.")
           )
```

We can clearly see that Logistic Regression performed worst. After tuning the parameters Boosting turned out to perform slightly better than the Random Forest in terms of AUC and Classification Error. Hence we would choose Boosting as the prefered model.
Whether 29.4% classification error is acceptable or not ultimately depends on the context. As we have seen earlier, 24.5% could also be achieved with a default random forest, but at the cost of a higher error for the "low" class.

Finally, let's check how the different variables influenced this result.

```{r}
h2o.varimp_plot(gbm.tuned, num_of_features = 11)
```

This ranking makes sense intuitively: Reviews and installs are an indication of how much an app is used. If an app has a large user base we would expect it to be a good app. Category is the second most important variable. It seems that in some categories the users are happier with the apps than in others. It also makes sense that size is highly related to the rating. The larger the app, the more features it probably has, the more useful or entertaining it probably is. I am surprised, however, that price has such a small impact because one might expect that an app is generally better the more it costs. A reason for this may be that most apps are free in this data set.


